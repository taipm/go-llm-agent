# Ph∆∞∆°ng √Ån T√≠ch H·ª£p OpenAI & Google Gemini

## üìä Ph√¢n T√≠ch So S√°nh

### 1. OpenAI Go SDK (`github.com/openai/openai-go`)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ SDK ch√≠nh th·ª©c, b·∫£o tr√¨ t·ªët (2.6k stars)
- ‚úÖ API design r·∫•t clean v·ªõi functional options pattern
- ‚úÖ H·ªó tr·ª£ ƒë·∫ßy ƒë·ªß: Chat, Streaming, Function calling, Azure OpenAI
- ‚úÖ Error handling t·ªët v·ªõi typed errors
- ‚úÖ Auto-retry, pagination, middleware support
- ‚úÖ Webhooks verification

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è API kh√° verbose (nhi·ªÅu struct nested)
- ‚ö†Ô∏è Ph·ª• thu·ªôc Go 1.22+ (omitzero semantics)
- ‚ö†Ô∏è Complex type system (unions, param.Opt[T])

**V√≠ d·ª• API:**
```go
client := openai.NewClient(option.WithAPIKey("..."))
completion, err := client.Chat.Completions.New(ctx, openai.ChatCompletionNewParams{
    Messages: []openai.ChatCompletionMessageParamUnion{
        openai.UserMessage("Hello"),
    },
    Model: shared.ChatModelGPT4o,
})
```

---

### 2. Google Gen AI Go SDK (`google.golang.org/genai`)

**∆Øu ƒëi·ªÉm:**
- ‚úÖ SDK ch√≠nh th·ª©c t·ª´ Google (825 stars)
- ‚úÖ API ƒë∆°n gi·∫£n, d·ªÖ d√πng
- ‚úÖ H·ªó tr·ª£ c·∫£ Gemini API & Vertex AI
- ‚úÖ Multimodal Live support (video/audio streaming)
- ‚úÖ Function calling, caching, batches
- ‚úÖ Lightweight, √≠t boilerplate

**Nh∆∞·ª£c ƒëi·ªÉm:**
- ‚ö†Ô∏è √çt mature h∆°n OpenAI SDK
- ‚ö†Ô∏è Documentation ch∆∞a ƒë·∫ßy ƒë·ªß
- ‚ö†Ô∏è Streaming API kh√°c bi·ªát

**V√≠ d·ª• API:**
```go
client, _ := genai.NewClient(ctx, &genai.ClientConfig{
    APIKey: apiKey,
    Backend: genai.BackendGeminiAPI,
})
result, _ := client.Models.GenerateContent(ctx, "gemini-2.0-flash", 
    []*genai.Content{{Parts: parts}}, nil)
```

---

## üéØ Chi·∫øn L∆∞·ª£c T√≠ch H·ª£p

### Nguy√™n T·∫Øc Thi·∫øt K·∫ø

1. **Unified API First** - T·∫•t c·∫£ providers d√πng CHUNG 1 interface, kh√°c bi·ªát ch·ªâ ·ªü constructor
2. **Zero breaking changes** - Backward compatible ho√†n to√†n v·ªõi v0.1.x
3. **Provider pattern** - M·ªói LLM l√† m·ªôt provider implementation c·ªßa `types.LLMProvider`
4. **Abstraction over vendor-specific features** - ·∫®n complexity c·ªßa t·ª´ng SDK ph√≠a sau
5. **Ease of Use** - Switch provider = ƒë·ªïi 1 d√≤ng kh·ªüi t·∫°o, code c√≤n l·∫°i gi·ªØ nguy√™n

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Application                         ‚îÇ
‚îÇ  Code gi·ªëng h·ªát nhau, CH·ªà ƒë·ªïi provider constructor          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          go-llm-agent Core (UNIFIED API)                    ‚îÇ
‚îÇ  types.LLMProvider interface (Chat, Stream methods)         ‚îÇ
‚îÇ  Agent.Chat(), Agent.ChatStream() - SAME for ALL providers  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ             ‚îÇ             ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Ollama  ‚îÇ   ‚îÇ OpenAI  ‚îÇ   ‚îÇ Gemini  ‚îÇ
         ‚îÇ Provider‚îÇ   ‚îÇ Provider‚îÇ   ‚îÇ Provider‚îÇ
         ‚îÇ (v0.1)  ‚îÇ   ‚îÇ (NEW)   ‚îÇ   ‚îÇ (NEW)   ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ             ‚îÇ            ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ  HTTP   ‚îÇ   ‚îÇ openai- ‚îÇ   ‚îÇgo-genai ‚îÇ
         ‚îÇ Client  ‚îÇ   ‚îÇ go v3.6.1‚îÇ   ‚îÇv1.32.0  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       (PINNED)      (PINNED)

Key: T·∫•t c·∫£ providers implement types.LLMProvider ‚Üí Code d√πng Agent KH√îNG bi·∫øt provider n√†o
```

---

## üì¶ Implementation Plan

### Phase 1: OpenAI Provider (∆Øu ti√™n cao)

**L√Ω do ∆∞u ti√™n:**
- OpenAI ph·ªï bi·∫øn nh·∫•t (GPT-4, GPT-4o)
- SDK mature, well-documented
- Production-ready

**Tasks:**

#### 1.1. Create OpenAI Provider Structure
```go
// pkg/provider/openai/openai.go
package openai

import (
    "context"
    "github.com/openai/openai-go/v3"
    "github.com/openai/openai-go/v3/option"
    "github.com/taipm/go-llm-agent/pkg/types"
)

type Provider struct {
    client *openai.Client
    model  string
}

func New(apiKey, model string) *Provider {
    client := openai.NewClient(option.WithAPIKey(apiKey))
    return &Provider{
        client: client,
        model:  model,
    }
}

func (p *Provider) Chat(ctx context.Context, messages []types.Message, options *types.ChatOptions) (*types.Response, error) {
    // Convert types.Message -> openai.ChatCompletionMessageParamUnion
    // Call client.Chat.Completions.New()
    // Convert openai.ChatCompletion -> types.Response
}

func (p *Provider) Stream(ctx context.Context, messages []types.Message, options *types.ChatOptions, handler types.StreamHandler) error {
    // Stream with client.Chat.Completions.NewStreaming()
    // Convert chunks and call handler
}
```

#### 1.2. Message Conversion Layer
```go
// Converter t·ª´ types.Message sang OpenAI format
func toOpenAIMessages(msgs []types.Message) []openai.ChatCompletionMessageParamUnion {
    result := make([]openai.ChatCompletionMessageParamUnion, len(msgs))
    for i, msg := range msgs {
        switch msg.Role {
        case types.RoleUser:
            result[i] = openai.UserMessage(msg.Content)
        case types.RoleAssistant:
            result[i] = openai.AssistantMessage(msg.Content)
        case types.RoleSystem:
            result[i] = openai.SystemMessage(msg.Content)
        }
        
        // Handle tool calls if present
        if len(msg.ToolCalls) > 0 {
            // Convert tool calls
        }
    }
    return result
}
```

#### 1.3. Tool Calling Support
```go
// Convert types.ToolDefinition -> openai.ChatCompletionToolParam
func toOpenAITools(tools []types.ToolDefinition) []openai.ChatCompletionToolParam {
    result := make([]openai.ChatCompletionToolParam, len(tools))
    for i, tool := range tools {
        result[i] = openai.ChatCompletionToolParam{
            Type: openai.ChatCompletionToolTypeFunction,
            Function: openai.FunctionDefinitionParam{
                Name:        openai.String(tool.Function.Name),
                Description: openai.String(tool.Function.Description),
                Parameters:  tool.Function.Parameters,
            },
        }
    }
    return result
}
```

**Estimated Effort:** 2-3 days

**Files to create:**
- `pkg/provider/openai/openai.go` (~300 lines)
- `pkg/provider/openai/converter.go` (~200 lines)
- `pkg/provider/openai/openai_test.go` (~400 lines)
- `examples/openai_chat/main.go` (~100 lines)

**Critical: API Uniformity Checklist**
- [ ] Constructor signature: `New(apiKey, model string) *Provider`
- [ ] Implements `types.LLMProvider` interface exactly
- [ ] `Chat(ctx, messages, options) (*Response, error)` - same signature
- [ ] `Stream(ctx, messages, options, handler) error` - same signature
- [ ] Error handling returns `types.ProviderError` for consistency
- [ ] Example code structure matches `examples/simple_chat/main.go`

---

### Phase 2: Google Gemini Provider

**L√Ω do th·ª© hai:**
- Gemini 2.0 Flash r·∫•t nhanh & mi·ªÖn ph√≠
- Multimodal capabilities (text, image, video, audio)
- Vertex AI support cho enterprise

**Tasks:**

#### 2.1. Gemini Provider Structure
```go
// pkg/provider/gemini/gemini.go
package gemini

import (
    "context"
    "google.golang.org/genai"
    "github.com/taipm/go-llm-agent/pkg/types"
)

type Provider struct {
    client *genai.Client
    model  string
}

func New(apiKey, model string) (*Provider, error) {
    client, err := genai.NewClient(context.Background(), &genai.ClientConfig{
        APIKey:  apiKey,
        Backend: genai.BackendGeminiAPI,
    })
    if err != nil {
        return nil, err
    }
    
    return &Provider{
        client: client,
        model:  model,
    }, nil
}

func (p *Provider) Chat(ctx context.Context, messages []types.Message, options *types.ChatOptions) (*types.Response, error) {
    // Convert to genai.Content
    // Call client.Models.GenerateContent()
    // Convert response
}
```

#### 2.2. Multimodal Support Extension
```go
// Extend types.Message ƒë·ªÉ support multimodal
type Part struct {
    Text      string          `json:"text,omitempty"`
    InlineData *Blob          `json:"inline_data,omitempty"`
}

type Blob struct {
    MIMEType string `json:"mime_type"`
    Data     []byte `json:"data"`
}

// User c√≥ th·ªÉ g·ª≠i:
msg := types.Message{
    Role: types.RoleUser,
    Parts: []types.Part{
        {Text: "What's in this image?"},
        {InlineData: &types.Blob{
            MIMEType: "image/jpeg",
            Data: imageBytes,
        }},
    },
}
```

**Estimated Effort:** 2-3 days

**Files to create:**
- `pkg/provider/gemini/gemini.go` (~250 lines)
- `pkg/provider/gemini/converter.go` (~150 lines)
- `pkg/provider/gemini/gemini_test.go` (~300 lines)
- `examples/gemini_chat/main.go` (~100 lines)
- `examples/gemini_multimodal/main.go` (~150 lines)

**Critical: API Uniformity Checklist**
- [ ] Constructor signature: `New(apiKey, model string) (*Provider, error)`
- [ ] Implements `types.LLMProvider` interface exactly
- [ ] Chat/Stream signatures match OpenAI and Ollama providers
- [ ] Basic text chat works identically to other providers
- [ ] Multimodal = optional extension, doesn't break base API
- [ ] Example `gemini_chat` mirrors `openai_chat` and `simple_chat`

---

### Phase 3: Unified Configuration & Factory

#### 3.1. Provider Factory Pattern
```go
// pkg/provider/factory.go
package provider

import (
    "fmt"
    "github.com/taipm/go-llm-agent/pkg/types"
    "github.com/taipm/go-llm-agent/pkg/provider/ollama"
    "github.com/taipm/go-llm-agent/pkg/provider/openai"
    "github.com/taipm/go-llm-agent/pkg/provider/gemini"
)

type ProviderType string

const (
    ProviderOllama  ProviderType = "ollama"
    ProviderOpenAI  ProviderType = "openai"
    ProviderGemini  ProviderType = "gemini"
)

type Config struct {
    Type     ProviderType
    APIKey   string
    BaseURL  string
    Model    string
}

func New(config Config) (types.LLMProvider, error) {
    switch config.Type {
    case ProviderOllama:
        return ollama.New(config.BaseURL, config.Model), nil
    case ProviderOpenAI:
        return openai.New(config.APIKey, config.Model), nil
    case ProviderGemini:
        return gemini.New(config.APIKey, config.Model)
    default:
        return nil, fmt.Errorf("unknown provider type: %s", config.Type)
    }
}
```

#### 3.2. Easy Setup cho User
```go
// examples/multi_provider/main.go
func main() {
    // C√°ch 1: Direct provider creation
    ollamaProvider := ollama.New("http://localhost:11434", "llama3.2")
    agent1 := agent.New(ollamaProvider)
    
    // C√°ch 2: Factory pattern
    provider, _ := provider.New(provider.Config{
        Type:   provider.ProviderOpenAI,
        APIKey: os.Getenv("OPENAI_API_KEY"),
        Model:  "gpt-4o",
    })
    agent2 := agent.New(provider)
    
    // C√°ch 3: From environment variables
    provider, _ := provider.FromEnv() // Auto-detect t·ª´ env vars
    agent3 := agent.New(provider)
}
```

**Estimated Effort:** 1 day
**Files to create:**
- `pkg/provider/factory.go` (~150 lines)
- `pkg/provider/factory_test.go` (~100 lines)
- `examples/multi_provider/main.go` (~200 lines)

---

## üîß Technical Decisions

### 1. Dependencies Management

**Strategy: Pinned Versions for Stability**

```go
// go.mod
module github.com/taipm/go-llm-agent

go 1.22 // Required by both openai-go and go-genai

require (
    github.com/openai/openai-go/v3 v3.6.1    // Pinned: Latest stable (Nov 2024)
    google.golang.org/genai v1.32.0          // Pinned: Latest stable (Oct 2025)
)
```

**Why Pinned Versions:**
- ‚úÖ **Stability**: Avoid breaking changes from SDK updates
- ‚úÖ **Reproducible builds**: Same behavior across environments
- ‚úÖ **Controlled upgrades**: Update when we validate compatibility
- ‚úÖ **Security**: Known versions, easier to audit

**Version Update Policy:**
1. Review SDK changelogs quarterly
2. Test in dev branch first
3. Update pin after validation
4. Document changes in CHANGELOG.md

**Trade-offs:**
- Users get all SDKs (larger package) ‚Üí OK for production use
- Single go.mod ‚Üí easier to maintain
- Consistent experience ‚Üí prioritizes ease of use

---

### 2. API Compatibility Matrix

| Feature | Ollama | OpenAI | Gemini | Support Strategy |
|---------|--------|--------|--------|------------------|
| Chat | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| Streaming | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| Function calling | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| System prompts | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| Temperature | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| Max tokens | ‚úÖ | ‚úÖ | ‚úÖ | Core feature |
| Vision (images) | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | Extended feature |
| Audio | ‚ùå | ‚úÖ | ‚úÖ | Extended feature |
| Video | ‚ùå | ‚ùå | ‚úÖ | Extended feature |
| JSON mode | ‚ö†Ô∏è | ‚úÖ | ‚úÖ | Extended feature |
| Caching | ‚ùå | ‚ùå | ‚úÖ | Provider-specific |

**Strategy:**
- Core features: Implement cho t·∫•t c·∫£ providers
- Extended features: Optional, graceful degradation
- Provider-specific: Document clearly, c√≥ th·ªÉ access raw client

---

### 3. Error Handling Strategy

```go
// pkg/types/errors.go
package types

type ProviderError struct {
    Provider   string
    StatusCode int
    Message    string
    Original   error
}

func (e *ProviderError) Error() string {
    return fmt.Sprintf("[%s] %s", e.Provider, e.Message)
}

// Usage
_, err := provider.Chat(ctx, messages, nil)
if err != nil {
    var provErr *types.ProviderError
    if errors.As(err, &provErr) {
        if provErr.StatusCode == 429 {
            // Rate limit
        }
    }
}
```

---

## üìù Documentation Updates

### README.md additions:

```markdown
## üåê Supported Providers

- **Ollama** - Run models locally (llama3, qwen, etc.)
- **OpenAI** - GPT-4, GPT-4o, GPT-3.5
- **Google Gemini** - Gemini 2.0 Flash, Pro, Ultra

### Quick Start with Different Providers

#### Ollama (Local)
\`\`\`go
provider := ollama.New("http://localhost:11434", "llama3.2")
agent := agent.New(provider)
\`\`\`

#### OpenAI
\`\`\`go
provider := openai.New(os.Getenv("OPENAI_API_KEY"), "gpt-4o")
agent := agent.New(provider)
\`\`\`

#### Google Gemini
\`\`\`go
provider, _ := gemini.New(os.Getenv("GEMINI_API_KEY"), "gemini-2.0-flash")
agent := agent.New(provider)
\`\`\`

### Provider Comparison

| Provider | Speed | Cost | Local | Multimodal |
|----------|-------|------|-------|------------|
| Ollama   | Fast  | Free | ‚úÖ    | ‚ö†Ô∏è         |
| OpenAI   | Fast  | $$   | ‚ùå    | ‚úÖ         |
| Gemini   | Fastest| $   | ‚ùå    | ‚úÖ         |
```

---

## üìÖ Implementation Timeline

### Sprint 1: OpenAI Provider (Week 1)
- [x] Research & design (1 day) - **Current**
- [ ] OpenAI provider implementation (2 days)
- [ ] Message/tool conversion layer (1 day)
- [ ] Tests & examples (1 day)
- [ ] Documentation (0.5 day)

### Sprint 2: Gemini Provider (Week 2)
- [ ] Gemini provider implementation (2 days)
- [ ] Multimodal support (1 day)
- [ ] Tests & examples (1 day)
- [ ] Documentation (0.5 day)

### Sprint 3: Integration & Polish (Week 3)
- [ ] Factory pattern (1 day)
- [ ] Unified configuration (1 day)
- [ ] Comprehensive tests (1 day)
- [ ] Examples for all providers (1 day)
- [ ] Final documentation (1 day)

**Total: 3 weeks to complete**

---

## üéØ Success Metrics

1. **Backward Compatibility**: 100% - existing code works unchanged
2. **Test Coverage**: Maintain >70% for all providers
3. **Documentation**: Clear examples for each provider
4. **Performance**: No degradation from direct SDK usage
5. **User Experience**: Simple 3-line provider switch

---

## üöÄ Example Usage (Final State) - API ƒê·ªìng Nh·∫•t

### C√°ch 1: Direct Provider (R√µ r√†ng, d·ªÖ debug)

```go
package main

import (
    "context"
    "github.com/taipm/go-llm-agent/pkg/agent"
    "github.com/taipm/go-llm-agent/pkg/provider/ollama"
    "github.com/taipm/go-llm-agent/pkg/provider/openai"
    "github.com/taipm/go-llm-agent/pkg/provider/gemini"
)

func main() {
    ctx := context.Background()
    
    // CH·ªà c·∫ßn ƒë·ªïi 1 D√íNG n√†y, code c√≤n l·∫°i GI·ªêNG H·ªÜT NHAU!
    
    // Option 1: Ollama (local, free)
    provider := ollama.New("http://localhost:11434", "llama3.2")
    
    // Option 2: OpenAI (GPT-4, cloud)
    // provider := openai.New(os.Getenv("OPENAI_API_KEY"), "gpt-4o")
    
    // Option 3: Gemini (fastest, cheapest)
    // provider, _ := gemini.New(os.Getenv("GEMINI_API_KEY"), "gemini-2.0-flash")
    
    // --- Code d∆∞·ªõi ƒë√¢y HO√ÄN TO√ÄN GI·ªêNG NHAU cho t·∫•t c·∫£ providers ---
    ag := agent.New(provider)
    
    response, err := ag.Chat(ctx, "What is the capital of France?")
    if err != nil {
        panic(err)
    }
    
    fmt.Println(response) // "Paris" - regardless of provider!
}
```

### C√°ch 2: Factory Pattern (Linh ho·∫°t, config-driven)

```go
func main() {
    ctx := context.Background()
    
    // Load t·ª´ env/config
    // PROVIDER=openai OPENAI_API_KEY=xxx go run .
    // PROVIDER=gemini GEMINI_API_KEY=xxx go run .
    // PROVIDER=ollama OLLAMA_BASE_URL=xxx go run .
    
    provider, err := provider.FromEnv()
    if err != nil {
        panic(err)
    }
    
    ag := agent.New(provider)
    response, _ := ag.Chat(ctx, "Hello!")
    fmt.Println(response)
}
```

### C√°ch 3: Advanced - Provider Fallback

```go
func createProviderWithFallback() types.LLMProvider {
    // Try OpenAI first
    if apiKey := os.Getenv("OPENAI_API_KEY"); apiKey != "" {
        return openai.New(apiKey, "gpt-4o")
    }
    
    // Fallback to Gemini
    if apiKey := os.Getenv("GEMINI_API_KEY"); apiKey != "" {
        provider, _ := gemini.New(apiKey, "gemini-2.0-flash")
        return provider
    }
    
    // Default to local Ollama
    return ollama.New("http://localhost:11434", "llama3.2")
}

func main() {
    provider := createProviderWithFallback()
    ag := agent.New(provider)
    // Rest is the same!
}
```

**üéØ ƒêi·ªÉm M·∫°nh:**
- ‚úÖ Switch provider = 1 d√≤ng code
- ‚úÖ API ƒë·ªìng nh·∫•t 100% - kh√¥ng c·∫ßn h·ªçc l·∫°i
- ‚úÖ Type-safe - compile-time check
- ‚úÖ Test d·ªÖ - mock provider interface

---

## ‚ö†Ô∏è Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| SDK breaking changes | High | Pin versions, vendor if needed |
| API differences | Medium | Abstraction layer, feature matrix |
| Increased complexity | Medium | Clear docs, examples |
| Package size growth | Low | Optional dependencies |
| Maintenance burden | High | Prioritize popular providers |

---

## üí° Future Enhancements (Post v0.2)

1. **Anthropic Claude** provider
2. **Azure OpenAI** specific provider
3. **Provider fallback/retry** - try OpenAI, fallback to Gemini
4. **Cost tracking** per provider
5. **Provider pooling** - load balance across multiple
6. **Streaming aggregation** - combine multiple provider streams

---

## üìå Decision Summary

**Recommended Approach:**
1. ‚úÖ **Pin SDK versions** - `openai-go@v3.6.1`, `go-genai@v1.32.0` for stability
2. ‚úÖ **Include all SDKs** - Users get consistent experience, easier maintenance
3. ‚úÖ **Unified API enforced** - All providers implement same interface exactly
4. ‚úÖ **Switch = 1 line change** - Only constructor differs, rest identical
5. ‚úÖ **100% backward compatible** - v0.1 code works unchanged
6. ‚úÖ **Ease of use first** - Hide SDK complexity completely

**Core Principles:**
- **API Uniformity** - H·ªçc 1 l·∫ßn, d√πng cho t·∫•t c·∫£ providers
- **Type Safety** - Interface contract enforced at compile time
- **Stability** - Pinned versions, controlled upgrades
- **Simplicity** - Provider switching transparent to user code

**Implementation Priority:**
1. **Week 1: OpenAI Provider** (v3.6.1 pinned)
   - Most popular, enterprise-ready
   - Validate unified API design
   
2. **Week 2: Gemini Provider** (v1.32.0 pinned)
   - Fast, cost-effective
   - Multimodal capabilities
   
3. **Week 3: Integration & Polish**
   - Factory pattern
   - Comprehensive tests
   - Migration guide

**Quality Gates (Each Provider):**
- [ ] Implements `types.LLMProvider` interface
- [ ] Constructor matches pattern: `New(apiKey, model string)`
- [ ] Chat/Stream work identically to Ollama provider
- [ ] Example code structure mirrors existing examples
- [ ] Tests match coverage level (~70%)
- [ ] Documentation shows provider switch = 1 line change

**Next Step:** 
Start implementing OpenAI provider with v3.6.1 following this plan!
