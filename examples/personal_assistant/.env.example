# ============================================
# Personal Assistant Configuration
# ============================================

# --------------------------------------------
# LLM Provider Settings
# --------------------------------------------
# Options: ollama, openai, gemini
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5:3b

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# OpenAI Configuration (uncomment if using OpenAI)
# OPENAI_API_KEY=sk-xxx
# OPENAI_BASE_URL=https://api.openai.com

# Gemini Configuration (uncomment if using Gemini)
# GEMINI_API_KEY=your-api-key-here

# Gemini Vertex AI (alternative to API key)
# GEMINI_PROJECT_ID=my-project
# GEMINI_LOCATION=us-central1

# --------------------------------------------
# Memory & Learning Settings
# --------------------------------------------
# Use VectorMemory for semantic search (requires Qdrant)
USE_VECTOR_MEMORY=true

# Qdrant Configuration
QDRANT_URL=localhost:6334
QDRANT_COLLECTION=personal_assistant

# Embedding Model (for VectorMemory)
EMBEDDING_MODEL=nomic-embed-text:latest
EMBEDDING_BASE_URL=http://localhost:11434

# Memory Cache Size
MEMORY_CACHE_SIZE=100

# --------------------------------------------
# Agent Behavior Settings
# --------------------------------------------
# Enable self-learning from interactions
ENABLE_LEARNING=true

# Enable self-reflection on answers
ENABLE_REFLECTION=true

# Minimum confidence threshold for reflection (0.0-1.0)
MIN_CONFIDENCE=0.7

# System Prompt (optional - uses default if not set)
# SYSTEM_PROMPT=You are a helpful AI assistant with access to powerful tools...

# --------------------------------------------
# Logging Settings
# --------------------------------------------
# Log level: DEBUG, INFO, WARN, ERROR
LOG_LEVEL=INFO

# --------------------------------------------
# Performance Settings
# --------------------------------------------
# LLM Temperature (0.0-1.0, lower = more focused)
TEMPERATURE=0.7

# Max tokens per response
MAX_TOKENS=2000

# Max tool calling iterations
MAX_ITERATIONS=10
